output_dir:

overwrite_output_dir: False

do_train: False
do_eval:
do_predict: False
evaluation_strategy: "no"  # IntervalStrategy = field(default="no", metadata={"help": "The evaluation strategy to use."},)
prediction_loss_only: False

per_device_train_batch_size:
per_device_eval_batch_size:

gradient_accumulation_steps: 1
eval_accumulation_steps:  # "Number of predictions steps to accumulate before moving the tensors to the CPU."},

learning_rate: 5e-5 # float = field(default=5e-5, metadata={"help": "The initial learning rate for AdamW."})
weight_decay: 0.0  # float = field(default=0.0, metadata={"help": "Weight decay for AdamW if we apply some."})
adam_beta1: 0.9 # float = field(default=0.9, metadata={"help": "Beta1 for AdamW optimizer"})
adam_beta2: 0.999 # float = field(default=0.999, metadata={"help": "Beta2 for AdamW optimizer"})
adam_epsilon: 1e-8 # float = field(default=1e-8, metadata={"help": "Epsilon for AdamW optimizer."})
max_grad_norm: 1.0 # float = field(default=1.0, metadata={"help": "Max gradient norm."})

num_train_epochs: 3.0 # float = field(default=3.0, metadata={"help": "Total number of training epochs to perform."})
max_steps: -1
lr_scheduler_type: "linear"
warmup_ratio: 0.0
warmup_steps: 0

logging_dir: # Optional[str] = field(default_factory=default_logdir, metadata={"help": "Tensorboard log dir."})
logging_strategy: "steps" # IntervalStrategy = field(default="steps", metadata={"help": "The logging strategy to use."},)
logging_first_step: False # bool = field(default=False, metadata={"help": "Log the first global_step"})
logging_steps: 500 # int = field(default=500, metadata={"help": "Log every X updates steps."})
save_strategy: "steps"
#save_strategy: IntervalStrategy = field(
#        default="steps",
#        metadata={"help": "The checkpoint save strategy to use."},
#    )

save_steps: 500 # int = field(default=500, metadata={"help": "Save checkpoint every X updates steps."})
save_total_limit: # Optional[int] = field(
#        default=None,
#        metadata={
#            "help": (
#                "Limit the total amount of checkpoints."
#                "Deletes the older checkpoints in the output_dir. Default is unlimited checkpoints"
#            )
#        },
#    )
no_cuda: False # bool = field(default=False, metadata={"help": "Do not use CUDA even when it is available"})
seed: 42 # int = field(default=42, metadata={"help": "Random seed that will be set at the beginning of training."})

fp16: False # bool = field(
#        default=False,
#        metadata={"help": "Whether to use 16-bit (mixed) precision instead of 32-bit"},
#    )
fp16_opt_level: "O1" # str = field(
#        default="O1",
#        metadata={
#            "help": (
#                "For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']."
#                "See details at https://nvidia.github.io/apex/amp.html"
#            )
#        },
#    )
fp16_backend: "amp" # str = field(
#        default="auto",
#        metadata={"help": "The backend to be used for mixed precision.", "choices": ["auto", "amp", "apex"]},
#    )
fp16_full_eval: False # bool = field(
#        default=False,
#        metadata={"help": "Whether to use full 16-bit precision evaluation instead of 32-bit"},
#    )
local_rank: -1 # int = field(default=-1, metadata={"help": "For distributed training: local_rank"})

tpu_num_cores: None # Optional[int] = field(
#        default=None, metadata={"help": "TPU: Number of TPU cores (automatically passed by launcher script)"}
#    )
tpu_metrics_debug: False # bool = field(
#        default=False,
#        metadata={"help": "Deprecated, the use of `--debug` is preferred. TPU: Whether to print debug metrics"},
#    )
debug: False # bool = field(default=False, metadata={"help": "Whether to print debug metrics on TPU"})

dataloader_drop_last: False # bool = field(
#        default=False, metadata={"help": "Drop the last incomplete batch if it is not divisible by the batch size."}
#    )
eval_steps: # int = field(default=None, metadata={"help": "Run an evaluation every X steps."})
dataloader_num_workers: 0 # int = field(
#        default=0,
#        metadata={
#            "help": "Number of subprocesses to use for data loading (PyTorch only). 0 means that the data will be loaded in the main process."
#        },
#    )

past_index: # int = field(
#        default=-1,
#        metadata={"help": "If >=0, uses the corresponding part of the output as the past state for next step."},
#    )

run_name: # Optional[str] = field(
#        default=None, metadata={"help": "An optional descriptor for the run. Notably used for wandb logging."}
#    )
disable_tqdm: # Optional[bool] = field(
#        default=None, metadata={"help": "Whether or not to disable the tqdm progress bars."}
#    )

remove_unused_columns: True # Optional[bool] = field(
#        default=True, metadata={"help": "Remove columns not required by the model when using an nlp.Dataset."}
#    )
label_names: # Optional[List[str]] = field(
#        default=None, metadata={"help": "The list of keys in your dictionary of inputs that correspond to the labels."}
#    )

load_best_model_at_end: False # Optional[bool] = field(
#        default=False,
#        metadata={"help": "Whether or not to load the best model found during training at the end of training."},
#    )
metric_for_best_model: # Optional[str] = field(
#        default=None, metadata={"help": "The metric to use to compare two different models."}
#    )
greater_is_better: # Optional[bool] = field(
#        default=None, metadata={"help": "Whether the `metric_for_best_model` should be maximized or not."}
#    )
ignore_data_skip: False # bool = field(
#        default=False,
#        metadata={
#            "help": "When resuming training, whether or not to skip the first epochs and batches to get to the same training data."
#        },
#    )
sharded_ddp: "" # str = field(
#        default="",
#        metadata={
#            "help": "Whether or not to use sharded DDP training (in distributed training only). The base option "
#            "should be `simple`, `zero_dp_2` or `zero_dp_3` and you can add CPU-offload to `zero_dp_2` or `zero_dp_3` "
#            "like this: zero_dp_2 offload` or `zero_dp_3 offload`. You can add auto-wrap to `zero_dp_2` or "
#            "with the same syntax: zero_dp_2 auto_wrap` or `zero_dp_3 auto_wrap`.",
#        },
#    )
deepspeed: # Optional[str] = field(
#        default=None,
#        metadata={
#            "help": "Enable deepspeed and pass the path to deepspeed json config file (e.g. ds_config.json) or an already loaded json file as a dict"
#        },
#    )
label_smoothing_factor: 0.0 # float = field(
#        default=0.0, metadata={"help": "The label smoothing epsilon to apply (zero means no label smoothing)."}
#    )
adafactor: False # bool = field(default=False, metadata={"help": "Whether or not to replace AdamW by Adafactor."})
group_by_length: False # bool = field(
#        default=False,
#        metadata={"help": "Whether or not to group samples of roughly the same length together when batching."},
#    )
length_column_name: "length" # Optional[str] = field(
#        default="length",
#        metadata={"help": "Column name with precomputed lengths to use when grouping by length."},
#    )
report_to: # Optional[List[str]] = field(
#        default=None, metadata={"help": "The list of integrations to report the results and logs to."}
#    )
ddp_find_unused_parameters: # Optional[bool] = field(
#        default=None,
#        metadata={
#            "help": "When using distributed training, the value of the flag `find_unused_parameters` passed to "
#            "`DistributedDataParallel`."
#        },
#    )
dataloader_pin_memory: True # bool = field(
#        default=True, metadata={"help": "Whether or not to pin memory for DataLoader."}
#    )
skip_memory_metrics: False # bool = field(
#        default=False, metadata={"help": "Whether or not to skip adding of memory profiler reports to metrics."}
#    )
_n_gpu: -1 # int = field(init=False, repr=False, default=-1)
mp_parameters: "" # str = field(
#        default="",
#        metadata={"help": "Used by the SageMaker launcher to send mp-specific args. Ignored in Trainer"},
#    )